# -*- coding: utf-8 -*-
"""NLP-Classification-code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1s6r6BgtPvkwryI-r0mn64Vkt2yYeyxC1

# Importing Libraries
"""

import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('stopwords')

import pandas as pd
import plotly.express as px

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import re
import string
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics  import f1_score,accuracy_score
from sklearn.metrics import  confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense
from tensorflow.keras.utils import to_categorical
from nltk.stem import PorterStemmer

"""# Splitting of Data"""

#importing the csv files downloaded from Kaggle
df_train = pd.read_csv("train.csv")
df_test = pd.read_csv("test.csv")

df_combined = pd.concat([df_train, df_test], ignore_index=True)

#Combined data set split in 80:20 ratio for train and test respectively
#stratify set on Class Index to ensure even distribution of the target class
df_train, df_test = train_test_split(df_combined, test_size=0.20, random_state=42,stratify=df_combined['Class Index'])

# Printing shape of the new train and test file
print(df_train.shape)
print(df_test.shape)

"""# Exploratory Data Analysis"""

# merging train and test dataframe for EDA
data = pd.merge(df_train, df_test, how = 'outer')
# Cross-checking the type of the Variable
type(data)

data.head(10)

# change the news class type from numeric to string

data['Class Index'] =data['Class Index'].replace([1, 2, 3, 4], ['World', 'Sports', 'Business', 'Sci/Tech'])

data.describe(include='all')

# counts of rows and columns
rows = len(data.axes[0])
cols = len(data.axes[1])
print('Total Columns in the Corpus:', cols)
print('Total Entries in the Corpus :', rows)

#Calculating the occurences of the Different classes
print('Occourences of Different Class')
data['Class Index'].value_counts()

# Visualization of Different Classes
fig = px.pie(data_frame = data, names = 'Class Index', hole = 0.5, title = 'News Classification',
             width = 640, height = 480, color_discrete_sequence = px.colors.sequential.Bluered)
fig.update_layout(title_x = 0.5, uniformtext_minsize = 30)
fig.show()

# check the duplicate combinations of News Articles (title and description together)
dup = data[['Title', 'Description']].duplicated().sum()
print('Number of duplicate values:', dup)

# check the null values for data
print('Checking for null values')
data.isnull().sum()

#concatenating Title and Description
data['test_string'] = data['Title'] + " " + data['Description']

data.head(10)

#Counting number of characters in each message
data['char'] = data['test_string'].apply(len)
#Counting number of words in each message
data['word'] = data.apply(lambda row: nltk.word_tokenize(row['test_string']), axis=1).apply(len)
#Counting number of sentences in each message.
data['sentence'] = data.apply(lambda row: nltk.sent_tokenize(row['test_string']), axis=1).apply(len)

data.describe()

data.head(10)

#Visualization for Pairplot of number of words, sentences , and messages
fig = sns.pairplot(data = data, hue = 'Class Index', height = 2, aspect = 1.5)
plt.show(fig)

import matplotlib.pyplot as plt
from wordcloud import WordCloud

# Assuming your corpus is stored in a DataFrame 'data' with a column 'class' indicating the class labels

classes = data['Class Index'].unique()  # Get all unique class labels

for class_label in classes:
    class_data = data[data['Class Index'] == class_label]['test_string']  # Filter data for the current class

    words_dict = {}  # Initialize an empty dictionary to store words and their frequencies for the current class

    for message in class_data:
        words = [word.lower() for word in word_tokenize(message) if word.lower() not in stopwords.words('english') and word.lower().isalpha()]

        for word in words:
            if word in words_dict:
                words_dict[word] += 1
            else:
                words_dict[word] = 1

    # Generate a word cloud for the current class
    word_cloud = WordCloud().generate_from_frequencies(words_dict)

    # Create a figure and an axis
    fig, ax = plt.subplots()

    # Display the word cloud
    ax.imshow(word_cloud)

    # Set the axis limits
    ax.set_axis_off()

    # Set the title for the current class
    ax.set_title(f'Class: {class_label}')

    # Show the word cloud
    plt.show()

data['string_len'] = data['test_string'].apply(len)
data['string_len'].describe()

f, ax = plt.subplots(2, 2, figsize = (18, 6))

sns.distplot(data[data['Class Index'] == 'World']['string_len'], bins = 20, ax = ax[0,0])
ax[0,0].set_xlabel('World News Length')

sns.distplot(data[data['Class Index'] == 'Sports']['string_len'], bins = 20, ax = ax[0,1])
ax[0,1].set_xlabel('Sports News Length')

sns.distplot(data[data['Class Index'] == 'Business']['string_len'], bins = 20, ax = ax[1,0])
ax[1,0].set_xlabel('Business News Length')

sns.distplot(data[data['Class Index'] == 'Sci/Tech']['string_len'], bins = 20, ax = ax[1,1])
ax[1,1].set_xlabel('Sci/Tech News Length')

plt.show()

"""# DATA PRE-PROCESSING"""

#working on the train and test split by concatenating Title and Description
df_train['X_train'] = df_train['Title'] + " " + df_train['Description']
df_train['y_train'] = df_train['Class Index'].apply(lambda x: x-1).values
df_test['X_test'] = df_test['Title'] + " " + df_test['Description']
df_test['y_test'] = df_test['Class Index'].apply(lambda x: x-1).values

def clean_and_tokenize(text):
  """
  Preprocesses text by lowercasing, removing HTML tags, punctuation, numbers, and tokenizing.

  Args:
    text: String containing the text to be cleaned.

  Returns:
    List of tokens from the cleaned text.
  """
  text = text.lower()
  text = re.sub('<.*?>', '', text)
  text = re.sub('[%s]' % re.escape(string.punctuation), '', text)
  text = re.sub('\w*\d\w*', '', text)
  return word_tokenize(text)

def stem_and_join(tokens):
  """
  Applies stemming to a list of tokens and joins them back into a string.

  Args:
    tokens: List of tokens to be stemmed.

  Returns:
    String with the stemmed tokens joined together.
  """
  stemmer = PorterStemmer()
  stemmed_tokens = [stemmer.stem(word) for word in tokens]
  return ' '.join(stemmed_tokens)

def lemmatize_and_join(tokens):
  """
  Applies lemmatization to a list of tokens and joins them back into a string.

  Args:
    tokens: List of tokens to be lemmatized.

  Returns:
    String with the lemmatized tokens joined together.
  """
  stop_words = set(stopwords.words('english'))
  filtered_tokens = [word for word in tokens if word not in stop_words]
  lemmatizer = WordNetLemmatizer()
  lemmatized_text = [lemmatizer.lemmatize(word) for word in filtered_tokens]
  return ' '.join(lemmatized_text)

def apply_tfidf_varied_features(max_features_array, df_train_column, df_test_column):
    # Store the results in a dictionary
    results = {}

    # Iterate through the array of max feature numbers
    for max_features in max_features_array:


        # Initialize the TfidfVectorizer
        tfidf = TfidfVectorizer(max_features=max_features, min_df=6)

        # Fit and transform on training data
        tfidf_features = tfidf.fit_transform(df_train_column)
        # Transform the testing data
        test_tfidf_features = tfidf.transform(df_test_column)

        # Convert to dense arrays
        train_array = tfidf_features.toarray()
        test_array = test_tfidf_features.toarray()

        # Store the results
        results[max_features] = {
            "train_array": train_array,
            "test_array": test_array
        }

    return results

import nltk
nltk.download('wordnet')

#Applying clean and tokenization to the X_train column

df_train['clean_text'] = df_train['X_train'].apply(clean_and_tokenize)
df_train['stemmed_text'] = df_train['clean_text'].apply(stem_and_join)
df_train['lemmatized_text'] = df_train['clean_text'].apply(lemmatize_and_join)

df_test['clean_text'] = df_test['X_test'].apply(clean_and_tokenize)
df_test['stemmed_text'] = df_test['clean_text'].apply(stem_and_join)
df_test['lemmatized_text'] = df_test['clean_text'].apply(lemmatize_and_join)

result_lem = apply_tfidf_varied_features((5000,10000,15000,20000), df_train['lemmatized_text'], df_test['lemmatized_text'])
result_stem = apply_tfidf_varied_features((5000,10000,15000,20000), df_train['stemmed_text'], df_test['stemmed_text'])

"""# MODEL IMPLEMENTATION

## NAIVE BAYES
"""

def evaluate_naive_bayes_classifier(result, df_train, df_test):
    best_accuracy = 0
    accuracies = []

    for max_features, data in result.items():
        # Extract train and test arrays
        train_array = data['train_array']
        test_array = data['test_array']
        y_train = df_train['y_train']
        y_test = df_test['y_test']

        # Train Naive Bayes classifier
        clf = MultinomialNB()
        clf.fit(train_array, y_train)

        # Predict on the test set and calculate accuracy
        predictions = clf.predict(test_array)
        accuracy = accuracy_score(y_test, predictions)
        accuracies.append(accuracy)

        # Update best model if current model has higher accuracy
        if accuracy > best_accuracy:
            best_accuracy = accuracy
            best_model = clf
            best_predictions = clf.predict(test_array)
            best_max_features = max_features

    # Plotting results
    plt.figure(figsize=(10, 6))
    plt.plot(list(result_lem.keys()), accuracies, marker='o')
    plt.xlabel('Max Features')
    plt.ylabel('Accuracy')
    plt.title('Accuracy vs Max Features in Naive Bayes Classifier')
    plt.grid(True)
    plt.show()

    return best_max_features, best_accuracy, best_model, best_predictions

def eval_model(y,y_pred):
    print("F1 score of the model")
    print(f1_score(y,y_pred,average='weighted'))
    print("Accuracy of the model")
    print(accuracy_score(y,y_pred))
    print("Accuracy of the model in percentage")
    print(round(accuracy_score(y,y_pred)*100,3),"%")

def confusion_mat(color,predictions):
    cof=confusion_matrix(df_test['y_test'], predictions)
    cof=pd.DataFrame(cof, index=[i for i in range(1,5)], columns=[i for i in range(1,5)])
    sns.set(font_scale=1.5)
    plt.figure(figsize=(8,8));

    sns.heatmap(cof, cmap=color,linewidths=1, annot=True,square=True, fmt='d', cbar=False,xticklabels=['World','Sports','Business','Science'],yticklabels=['World','Sports','Business','Science']);
    plt.xlabel("Predicted Classes");
    plt.ylabel("Actual Classes");

"""### For Lemmatization"""

#Plotting Graph for the Accuracy for different Features selected
best_max_features_lem, best_accuracy_lem, best_model_lem,best_predictions_lem = evaluate_naive_bayes_classifier(result_lem, df_train, df_test)
print(f"Best max_features: {best_max_features_lem} with accuracy: {best_accuracy_lem}")

#F1-Score and Accuracy of the Model
eval_model(df_test['y_test'],best_predictions_lem)

# Creation of Confusion Matrix for Naive Bayes Lemmatization
confusion_mat('YlGnBu',best_predictions_lem)

"""### For Stemming"""

#Plotting Graph for the Accuracy for different Features selected
best_max_features_stem, best_accuracy_stem, best_model_stem,best_predictions_stem = evaluate_naive_bayes_classifier(result_stem, df_train, df_test)
print(f"Best max_features: {best_max_features_stem} with accuracy: {best_accuracy_stem}")

#F1-Score and Accuracy of the Model
eval_model(df_test['y_test'],best_predictions_stem)

# Creation of Confusion Matrix for Naive Bayes Stemming
confusion_mat('YlGnBu',best_predictions_stem)

"""## LONG SHORT-TERM MEORY NETWORK"""

import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense
from tensorflow.keras.utils import to_categorical

"""### For Stemming"""

tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(df_train['stemmed_text'])
sequences = tokenizer.texts_to_sequences(df_train['stemmed_text'])
X_train = pad_sequences(sequences, maxlen=500)
y_train = to_categorical(df_train['y_train'])
test_sequences = tokenizer.texts_to_sequences(df_test['stemmed_text'])
X_test = pad_sequences(test_sequences, maxlen=500)
y_test = to_categorical(df_test['y_test'])

model = Sequential()
model.add(Embedding(5000, 128, input_length=500))
model.add(LSTM(128, dropout=0.2))
model.add(Dense(4, activation='softmax'))

model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

model.fit(X_train, y_train, batch_size=128, epochs=3, validation_split=0.1)

loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {accuracy*100:.2f}%")

y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test, axis=1)
eval_model(y_true,y_pred_classes)

confusion_mat('YlGnBu',y_pred_classes)

'''
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test, axis=1)

cm = confusion_matrix(y_true, y_pred_classes)
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.title('Confusion Matrix')
plt.show()
'''

"""### For Lemmatization"""

tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(df_train['lemmatized_text'])
sequences = tokenizer.texts_to_sequences(df_train['lemmatized_text'])
X_train = pad_sequences(sequences, maxlen=500)
y_train = to_categorical(df_train['y_train'])
test_sequences = tokenizer.texts_to_sequences(df_test['lemmatized_text'])
X_test = pad_sequences(test_sequences, maxlen=500)
y_test = to_categorical(df_test['y_test'])

model.fit(X_train, y_train, batch_size=128, epochs=3, validation_split=0.1)

loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {accuracy*100:.2f}%")

y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test, axis=1)
eval_model(y_true,y_pred_classes)

confusion_mat('YlGnBu',y_pred_classes)

"""## BIDIRECTIONAL ENCODER REPRESENTATIONS FROM TRANSFORMERS

### For Lemmatization
"""

from transformers import BertTokenizer, BertForSequenceClassification
from torch.utils.data import DataLoader, Dataset
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from torch import nn, optim
import torch

# Assuming you have a DataFrame 'train_df' with columns 'lemmatized_text' and 'y_train'

# Split the data into training and validation sets
train_data, val_data, train_labels, val_labels = train_test_split(
    df_train['lemmatized_text'], df_train['y_train'], test_size=0.2, random_state=42
)

# Load pre-trained BERT tokenizer and model
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(df_train['y_train'].unique()))

# Tokenize the input data
train_tokens = tokenizer(list(train_data), padding=True, truncation=True, return_tensors='pt')
val_tokens = tokenizer(list(val_data), padding=True, truncation=True, return_tensors='pt')

# Create PyTorch Dataset
class NewsDataset(Dataset):
    def __init__(self, tokens, labels):
        self.tokens = tokens
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return {
            'input_ids': self.tokens['input_ids'][idx],
            'attention_mask': self.tokens['attention_mask'][idx],
            'labels': torch.tensor(self.labels.iloc[idx], dtype=torch.long)
        }

train_dataset = NewsDataset(train_tokens, train_labels)
val_dataset = NewsDataset(val_tokens, val_labels)

# Create DataLoader
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

# Define training parameters
optimizer = optim.AdamW(model.parameters(), lr=2e-5)
criterion = nn.CrossEntropyLoss()

# Train the model
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

num_epochs = 5
for epoch in range(num_epochs):
    model.train()
    for batch in train_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        optimizer.zero_grad()
        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()

    # Evaluate on the validation set
    model.eval()
    val_predictions = []
    val_true_labels = []

    with torch.no_grad():
        for batch in val_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
            logits = outputs.logits
            _, predictions = torch.max(logits, dim=1)

            val_predictions.extend(predictions.cpu().numpy())
            val_true_labels.extend(labels.cpu().numpy())

    # Calculate validation metrics
    accuracy_val = accuracy_score(val_true_labels, val_predictions)
    conf_matrix_val = confusion_matrix(val_true_labels, val_predictions)
    classification_report_val = classification_report(val_true_labels, val_predictions)

    print(f"Epoch {epoch + 1}/{num_epochs}, Validation Accuracy: {accuracy_val}")
    print(f"Confusion Matrix (Validation):\n{conf_matrix_val}")
    print(f"Classification Report (Validation):\n{classification_report_val}")

def confusion_mat(color,true_label,predictions):
    cof=confusion_matrix(true_label, predictions)
    cof=pd.DataFrame(cof, index=[i for i in range(1,5)], columns=[i for i in range(1,5)])
    sns.set(font_scale=1.5)
    plt.figure(figsize=(8,8));

    sns.heatmap(cof, cmap=color,linewidths=1, annot=True,square=True, fmt='d', cbar=False,xticklabels=['World','Sports','Business','Science'],yticklabels=['World','Sports','Business','Science']);
    plt.xlabel("Predicted Classes");
    plt.ylabel("Actual Classes");

from transformers import BertTokenizer, BertForSequenceClassification
import torch
from torch.utils.data import DataLoader, Dataset
from sklearn.metrics import accuracy_score, confusion_matrix

# Load the saved model
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)
model.load_state_dict(torch.load('/content/drive/MyDrive/bert_model_lem.pth'))
model.eval()

# Assuming you have a DataFrame 'test_df' with columns 'lemmatized_text' and 'y_test'

# Tokenize the test data
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
test_tokens = tokenizer(list(df_test['lemmatized_text']), padding=True, truncation=True, return_tensors='pt')

# Create PyTorch Dataset for test data
class NewsDataset(Dataset):
    def __init__(self, tokens, labels):
        self.tokens = tokens
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return {
            'input_ids': self.tokens['input_ids'][idx],
            'attention_mask': self.tokens['attention_mask'][idx],
            'labels': torch.tensor(self.labels.iloc[idx], dtype=torch.long)
        }

test_dataset = NewsDataset(test_tokens, df_test['y_test'])

# Create DataLoader for test data
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# Evaluate the model on test data
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

test_predictions = []
test_true_labels = []

with torch.no_grad():
    for batch in test_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        logits = outputs.logits
        _, predictions = torch.max(logits, dim=1)

        test_predictions.extend(predictions.cpu().numpy())
        test_true_labels.extend(labels.cpu().numpy())

# Calculate test metrics

eval_model(test_true_labels, test_predictions)
confusion_mat('YlGnBu',test_true_labels,test_predictions)

"""### For Stemming"""

# Assuming you have a DataFrame 'train_df' with columns 'stemmed_text' and 'y_train'

# Split the data into training and validation sets
train_data, val_data, train_labels, val_labels = train_test_split(
    df_train['stemmed_text'], df_train['y_train'], test_size=0.2, random_state=42
)

# Load pre-trained BERT tokenizer and model
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(df_train['y_train'].unique()))

# Tokenize the input data
train_tokens = tokenizer(list(train_data), padding=True, truncation=True, return_tensors='pt')
val_tokens = tokenizer(list(val_data), padding=True, truncation=True, return_tensors='pt')

# Create PyTorch Dataset
class NewsDataset(Dataset):
    def __init__(self, tokens, labels):
        self.tokens = tokens
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return {
            'input_ids': self.tokens['input_ids'][idx],
            'attention_mask': self.tokens['attention_mask'][idx],
            'labels': torch.tensor(self.labels.iloc[idx], dtype=torch.long)
        }

train_dataset = NewsDataset(train_tokens, train_labels)
val_dataset = NewsDataset(val_tokens, val_labels)

# Create DataLoader
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

# Define training parameters
optimizer = optim.AdamW(model.parameters(), lr=2e-5)
criterion = nn.CrossEntropyLoss()

# Train the model
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

num_epochs = 5
for epoch in range(num_epochs):
    model.train()
    for batch in train_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        optimizer.zero_grad()
        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()

    # Evaluate on the validation set
    model.eval()
    val_predictions = []
    val_true_labels = []

    with torch.no_grad():
        for batch in val_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
            logits = outputs.logits
            _, predictions = torch.max(logits, dim=1)

            val_predictions.extend(predictions.cpu().numpy())
            val_true_labels.extend(labels.cpu().numpy())

    # Calculate validation metrics
    accuracy_val = accuracy_score(val_true_labels, val_predictions)
    conf_matrix_val = confusion_matrix(val_true_labels, val_predictions)
    classification_report_val = classification_report(val_true_labels, val_predictions)

    print(f"Epoch {epoch + 1}/{num_epochs}, Validation Accuracy: {accuracy_val}")
    print(f"Confusion Matrix (Validation):\n{conf_matrix_val}")
    print(f"Classification Report (Validation):\n{classification_report_val}")

from transformers import BertTokenizer, BertForSequenceClassification
import torch
from torch.utils.data import DataLoader, Dataset
from sklearn.metrics import accuracy_score, confusion_matrix

# Load the saved model
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)
model.load_state_dict(torch.load('/content/drive/MyDrive/bert_model_stem.pth'))
model.eval()

# Assuming you have a DataFrame 'test_df' with columns 'lemmatized_text' and 'y_test'

# Tokenize the test data
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
test_tokens = tokenizer(list(df_test['lemmatized_text']), padding=True, truncation=True, return_tensors='pt')

# Create PyTorch Dataset for test data
class NewsDataset(Dataset):
    def __init__(self, tokens, labels):
        self.tokens = tokens
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return {
            'input_ids': self.tokens['input_ids'][idx],
            'attention_mask': self.tokens['attention_mask'][idx],
            'labels': torch.tensor(self.labels.iloc[idx], dtype=torch.long)
        }

test_dataset = NewsDataset(test_tokens, df_test['y_test'])

# Create DataLoader for test data
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# Evaluate the model on test data
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

test_predictions = []
test_true_labels = []

with torch.no_grad():
    for batch in test_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        logits = outputs.logits
        _, predictions = torch.max(logits, dim=1)

        test_predictions.extend(predictions.cpu().numpy())
        test_true_labels.extend(labels.cpu().numpy())

# Calculate test metrics

eval_model(test_true_labels, test_predictions)
confusion_mat('YlGnBu',test_true_labels,test_predictions)

